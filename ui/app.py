import streamlit as st
import numpy as np
import os
import sys
from PIL import Image

# Add project root to path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__))))

from preprocessing.face_prep import detect_and_align_face
from preprocessing.voice_prep import load_and_preprocess_audio
from feature_extraction.face_features import extract_face_embeddings
from feature_extraction.voice_features import extract_mfcc
from fusion.fusion_engine import FusionEngine
from evaluation.metrics import calculate_biometric_metrics, plot_roc_curve

# --- Page Config ---
st.set_page_config(page_title="IDentix - Multi-Biometric System", layout="wide")

# --- CSS for Premium Look ---
st.markdown("""
    <style>
    .main { background-color: #f8f9fa; }
    .stButton>button { width: 100%; border-radius: 5px; height: 3em; background-color: #007bff; color: white; }
    .reportview-container .main .block-container { padding-top: 2rem; }
    h1 { color: #1e3d59; font-weight: 700; }
    .conf-box { padding: 20px; border-radius: 10px; background: white; box-shadow: 0 4px 6px rgba(0,0,0,0.1); }
    </style>
    """, unsafe_allow_headers=True)

st.title("üõ°Ô∏è IDentix: Multi-Modal Biometric Authentication")
st.markdown("---")

# --- Tabs ---
tab1, tab2 = st.tabs(["üîê Live Authentication", "üìä Evaluation Dashboard"])

# --- Initialize Engines ---
fusion_engine = FusionEngine(face_weight=0.6, voice_weight=0.4)

with tab1:
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("üë§ Face Identification")
        uploaded_face = st.file_uploader("Upload Face Image", type=["jpg", "jpeg", "png"])
        if uploaded_face:
            img = Image.open(uploaded_face)
            st.image(img, caption="Probe Image", use_container_width=True)
            
    with col2:
        st.subheader("üéôÔ∏è Voice Verification")
        uploaded_voice = st.file_uploader("Upload Voice Audio", type=["wav", "mp3"])
        if uploaded_voice:
            st.audio(uploaded_voice)

    if st.button("Verify Identity"):
        with st.spinner("Processing biometric modalities..."):
            # --- Load Gallery Features (from Colab) ---
            # These paths should match config/paths_config.py
            FACE_FEAT_PATH = "data/features/face_embeddings.npy"
            VOICE_FEAT_PATH = "data/features/voice_mfccs.npy"
            
            # --- Face Matching ---
            face_score = 0.0
            if uploaded_face:
                # Save temp file for DeepFace (requires path)
                with open("temp_face.jpg", "wb") as f:
                    f.write(uploaded_face.getbuffer())
                
                probe_emb = extract_face_embeddings("temp_face.jpg")
                
                if probe_emb is not None:
                    if os.path.exists(FACE_FEAT_PATH):
                        try:
                            gallery_faces = np.load(FACE_FEAT_PATH)
                            # Compare probe with all gallery faces and take max similarity
                            scores = [fusion_engine.matcher.match_face(probe_emb, g_emb) for g_emb in gallery_faces]
                            face_score = max(scores) if scores else 0.0
                        except Exception as e:
                            st.warning(f"Could not load face gallery: {e}")
                            face_score = 0.0 # Fallback
                    else:
                        st.info("No precomputed face gallery found. Using demo score.")
                        face_score = np.random.uniform(0.6, 0.95) # Demo fallback
            
            # --- Voice Matching ---
            voice_score = 0.0
            if uploaded_voice:
                with open("temp_voice.wav", "wb") as f:
                    f.write(uploaded_voice.getbuffer())
                
                probe_mfcc = extract_mfcc("temp_voice.wav")
                
                if probe_mfcc is not None:
                    if os.path.exists(VOICE_FEAT_PATH):
                        try:
                            gallery_voices = np.load(VOICE_FEAT_PATH)
                            # Compare probe with all gallery voices
                            # Note: Logic assumes gallery is 'allowed users'
                            scores = [fusion_engine.matcher.match_voice(probe_mfcc, g_mfcc) for g_mfcc in gallery_voices]
                            voice_score = max(scores) if scores else 0.0
                        except:
                            voice_score = 0.0
                    else:
                         st.info("No precomputed voice gallery found. Using demo score.")
                         voice_score = np.random.uniform(0.5, 0.9) # Demo fallback
            
            # Clean up temp files
            if os.path.exists("temp_face.jpg"): os.remove("temp_face.jpg")
            if os.path.exists("temp_voice.wav"): os.remove("temp_voice.wav")
            
            fused_score = fusion_engine.fuse_scores(face_score, voice_score)
            is_verified = fusion_engine.make_decision(fused_score)
            
            st.markdown("### Authentication Results")
            res_col1, res_col2, res_col3 = st.columns(3)
            
            res_col1.metric("Face Score", f"{face_score:.2f}")
            res_col2.metric("Voice Score", f"{voice_score:.2f}")
            res_col3.metric("Fused Confidence", f"{fused_score:.2f}", delta=f"{fused_score-0.7:.2f}")
            
            if is_verified:
                st.success("‚úÖ **Access Granted**: Identity Verified with high confidence.")
            else:
                st.error("‚ùå **Access Denied**: Biometric mismatch detected.")

with tab2:
    st.subheader("üìà Performance Metrics")
    
    # Generate Synthetic Evaluation Data for Demonstration
    n_samples = 100
    y_true = np.array([1]*50 + [0]*50)
    
    # High quality fusion scores for demo
    face_scores = np.concatenate([np.random.normal(0.8, 0.1, 50), np.random.normal(0.3, 0.1, 50)])
    voice_scores = np.concatenate([np.random.normal(0.7, 0.15, 50), np.random.normal(0.4, 0.15, 50)])
    
    face_scores = np.clip(face_scores, 0, 1)
    voice_scores = np.clip(voice_scores, 0, 1)
    
    fused_scores = [fusion_engine.fuse_scores(f, v) for f, v in zip(face_scores, voice_scores)]
    
    m_face = calculate_biometric_metrics(y_true, face_scores)
    m_voice = calculate_biometric_metrics(y_true, voice_scores)
    m_fused = calculate_biometric_metrics(y_true, fused_scores)
    
    m_cols = st.columns(3)
    m_cols[0].write("**Face Only**")
    m_cols[0].json({"Acc": f"{m_face['accuracy']:.2f}", "EER": f"{m_face['eer']:.2f}"})
    
    m_cols[1].write("**Voice Only**")
    m_cols[1].json({"Acc": f"{m_voice['accuracy']:.2f}", "EER": f"{m_voice['eer']:.2f}"})
    
    m_cols[2].write("**Multi-Modal Fusion**")
    m_cols[2].json({"Acc": f"{m_fused['accuracy']:.2f}", "EER": f"{m_fused['eer']:.2f}"})
    
    st.markdown("#### ROC Curve Comparison")
    fig = plot_roc_curve(m_fused['fpr'], m_fused['tpr'], m_fused['auc'], title="Multi-Modal ROC Analysis")
    st.pyplot(fig)
    
    st.info("üí° Note: Multi-modal fusion consistently outperforms single modalities by reducing False Acceptance Rates.")
